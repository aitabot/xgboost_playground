{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost for Timeseries\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "Ensemble models are a standard tool for predictive modeling and boosting is one technique to create ensemble models.\n",
    "\n",
    "Boosting fits a series of models and fits each successive model in order to minimize the error of the previous models.\n",
    "\n",
    "There are a couple of variants of this concept, one being gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "https://xgboost.readthedocs.io/\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework.\n",
    "\n",
    "XGBoost is an ensemble of decision trees where new trees fix errors of the trees that are already part of the model. Trees are added until no further improvements can be made to the model.\n",
    "\n",
    "Requirements to use XGBoost for time series:\n",
    "- evaluate the model via walk-forward validation, instead of k-fold cross validation, as k-fold would have biased results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pipenv install scikit-learn xgboost --skip-lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/MSFT-1Y-Hourly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"close\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform this to a supervised learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = df.close.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, perc):\n",
    "    data = data.values\n",
    "    n = int(len(data) * (1 - perc))\n",
    "    return data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the XGBRegressor class to make a prediction. XGBRegressor is an implementation of the scikit-learn API for XGBoost regression.\n",
    "\n",
    "We'll take the train set and test input row as input, fit a model, and make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[:, :-1]\n",
    "y = train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=1000)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test[0,0]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.array(test[0, 0]).reshape(1, -1)\n",
    "\n",
    "pred = model.predict(val)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict\n",
    "Train on train set and predict one sample at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_predict(train, test):\n",
    "    train = np.array(train)\n",
    "    X, y = train[:, :-1], train[:, -1]\n",
    "    model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=1000)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    val = np.array(test).reshape(1, -1)\n",
    "    pred = model.predict(val)\n",
    "    return train, X, y, pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, x, y, pred = xgb_predict(train, test[0, 0])\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walk-forward validation\n",
    "\n",
    "Since we are making a one step forward prediction, in this case an hourly prediction we will predict the first record in the test dataset. \n",
    "\n",
    "Afterwards we add the real observation from the test set to the train set, refit the model, then predict the next step in the test dataset.\n",
    "\n",
    "We'll evaluate the model with the RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def validate(data, perc):\n",
    "    predictions = []\n",
    "\n",
    "    train, test = train_test_split(data, perc)\n",
    "\n",
    "    history = [x for x in train]\n",
    "\n",
    "    for i in range(len(test)):\n",
    "        test_X, test_y = test[i, :-1], test[i, -1]\n",
    "\n",
    "        pred = xgb_predict(history, test_X[0])\n",
    "        predictions.append(pred)\n",
    "\n",
    "        history.append(test[i])\n",
    "\n",
    "    error = mean_squared_error(test[:, -1], predictions, squared=False)\n",
    "\n",
    "    return error, test[:, -1], predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rmse, y, pred = validate(df, 0.2)\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
